{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from nilearn import plotting\n",
    "from fragmenter import Fragment\n",
    "from fragmenter import adjacency\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple demo of the parcellation fragmenter, from simple use to some benchmarks. The first step is to load a template surface (in this case, a random participant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision\n",
    "\n",
    " *  Examine connectivity metrics as a function of cortical map resolution (# clusters)\n",
    "     * For example, iteratively sub-parcellate exising regions and examine if metrics break down, plateau etc.\n",
    "     \n",
    " * Use tractography or correlation profiles of sub-regions as higher-resolution features in machine learning algorithms\n",
    "     * Caveat: requires preprocessing step to \"match\" parcels across subjects (Hungarian algorithm)\n",
    "     * http://www.math.harvard.edu/archive/20_spring_05/handouts/assignment_overheads.pdf\n",
    "     \n",
    "     \n",
    " * Decrease processing time for some tools\n",
    "     * Searchlight FDR -- rather than searching over all vertices, search over parcels instead\n",
    "     * Modularity on 5000 parcels versus 32,000 vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surfaces are a 2D triangular mesh. When loading a surface, you get a tuple of two arrays: the 3D-space coordinates of each vertex and a list of triangles that represent the surface, such that each triangle is composed of 3 vertices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FreeSurfer mesh we're using here has ~164K vertices (fsaverage). Note that this package also accepts other standards, such as the Human Connectome Project's surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices,faces = nb.freesurfer.io.read_geometry('../data/freesurfer/fsaverage/surf/lh.sphere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n_vertices: {:}'.format(vertices.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parcellate a surface, we derive the neighbors of each vertex. You can consider this to be an adjacency matrix that allows for standard clustering methods to be computed on it. The `adjacency` function will give you this list (in this example, we will just look at the neighbors of vertex 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a surface adjacency object\n",
    "M = adjacency.SurfaceAdjacency(vertices=vertices, faces=faces)\n",
    "\n",
    "# Generate adjacency list\n",
    "M.generate()\n",
    "\n",
    "# Visualize\n",
    "print('Neighbors of vertex 1: ')\n",
    "print(M.adj[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fragment a surface, we first create a `Fragment` object that takes in the desired number of clusters, and whether you want to use pretty colors (True/False). In this case, we will run a simple 10 parcel example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_brain = Fragment.Fragment(n_clusters=75, use_pretty_colors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Fragment object is instantiated, you can see that it now has an attribute called `n_clusters` containing the number of clusters to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numer of clusters to generate: {:}'.format(whole_brain.n_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote the Fragment class to follow the sklearn format with methods like `fit` and `predict`.  We're going to fit the fragmentation method, feeding in the vertices and faces.  The following example uses k-means clustering for parcellating our surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_brain.fit(vertices = vertices, faces=faces, method = 'k_means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will add an attribute to `whole_brain` called `label_`, which is the fragmented cortical map, where each vertex is now assigned a new label value. We can confirm this by checking that length of the resulting vector is equal to the number of vertices in fsaverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# Generated Clusters: {:}'.format(np.unique(whole_brain.label_).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot the 75 parcels onto a surface using Nilearn's ```view_surf``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflated_vertices, inflated_faces = nb.freesurfer.io.read_geometry(\n",
    "        '../data/freesurfer/fsaverage/surf/lh.inflated')\n",
    "plotting.view_surf([inflated_vertices,inflated_faces], whole_brain.label_,threshold=0.1,cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcellating the whole brain is haphazard, and doesn't respect any previous anatomical boundaries.  We wanted to be able to parcellate individual regions as well. To do this, we first extract the vertices associated with each region using the `RegionExtractor` class -- we represent this output as a dictionary structure mapping region names to region indices -- we can then feed this dictionary back into out `Fragment` object, along with a list of ROIs we're interested in parcellating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specifically parcellate the temporal lobe and inferior parietal region, respecting the original boundaries by the the Desikan-Killiany atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fragmenter import RegionExtractor\n",
    "\n",
    "# Load atlas\n",
    "label_file = '../data/freesurfer/fsaverage/label/lh.aparc.annot'\n",
    "\n",
    "# Extract the vertices associated with each region, pick areas to parcellate\n",
    "E = RegionExtractor.Extractor(label_file)\n",
    "parcels = E.map_regions()\n",
    "\n",
    "# Define a list of regions of interest\n",
    "rois=['temporalpole','inferiortemporal','middletemporal','superiortemporal',\n",
    "      'transversetemporal','bankssts',\n",
    "      'inferiorparietal','supramarginal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model again, this time proving the `parcels` dictionary and `rois` list.  This time, let's specifiy that we want each generated parcel to have a specific `size` -- in this case, defined by the number of vertices.  If you know the length of each edge in *mm*, you can length parameterize fragment size by surface area, to make more biologically meaningful sub-regions (note that this option overrides `n_clusters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = Fragment.Fragment(n_clusters=75)\n",
    "temporal.fit(vertices=vertices,faces=faces,parcels=parcels,rois=rois,size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we plot this new label map, we'll see that we have independently fragmented each of the specified Desikan Killiany Regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_surf([inflated_vertices,inflated_faces],temporal.label_,threshold=0.1,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to use a pretty colormap to plot these new labels, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fragmenter import colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[keys, ctab, names, remapped] = colormaps.get_ctab_and_names(vertices,temporal.label_)\n",
    "plotting.view_surf([inflated_vertices,inflated_faces], remapped, threshold=0.1,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sub-region colors then follow a gradient determined by the superior-inferior axis.  If we want to save the generated map, we can save the label to a FreeSurfer annotation file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_name = '../data/freesurfer/temporal.annot'\n",
    "temporal.write(annot_name, use_pretty_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you want to save the label maps to export and use independently (say, in HCP's workbench, or in R), you can simply write them to a csv file. This can be useful if you are reducing the dimensionality of the cortex in order to run community detection algorithms in igraph (R version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for external use\n",
    "np.savetxt(\"WBLabels.csv\", whole_brain.label_, '%5.0f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were fragmenting the HCP surface (e.g. 32k), you can produce a CIFTI dscalar.nii file to view in the workbench using the wb_command utilities. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` {bash}\n",
    "wb_command -cifti-convert -from-text WBLabels.csv 100307.MyelinMap_BC.32k_fs_LR.dscalar.nii WBLabels.dscalar.nii\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the `cifti-convert` function requires an example file to generate the new one (in this case, 100307's 32k myelin map)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of the base fragmentation code, we've also developed some methods to generate \"null\" parcellations. These produce slight shifts of the location of the labels either through an affine rotation or probabilistic remapping of edges. If you would like to learn more about the algorithms used to fraction the surfaces, see the benchmark notebook for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    " *  Developed a fast way to generate new cortical maps for use in null models, and higher-resolution models\n",
    " *  Could easily fit into typical connectivity analysis pipelines for generating regional connectivity matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things We Learned\n",
    "\n",
    " *  **TONS** of Git -- hopefully you never find youself in a situation where your local clone and repo histories don't align (thanks Kirstie for the help!!)\n",
    " *  moving from R to Python\n",
    " *  writing object-oriented code\n",
    " *  properly packaging and distributing software\n",
    " *  collaborative coding -- hugely beneficial -- we all felt that this was invaluable to improving our own programming skills, question our biases for how to go about solving problems\n",
    " *  ever-so-slight inspiration for how to use Javascript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks to\n",
    " *  Ariel, Tal, and Kirstie\n",
    " *  Michael and Ross for the inspiration and Git help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
